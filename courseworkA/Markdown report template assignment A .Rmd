---
title: "Report coursework assignment A - 2021"
subtitle: "CS4125 Seminar Research Methodology for Data Science"
author: "Nikki Bouman (4597648), Anuj Singh (5305926), Gwennan Smitskamp (4349822)"
date: "20/04/2021"
output:
   pdf_document:
      fig_caption: true
      number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


\tableofcontents

```{r}
library(foreign)
library(ggplot2)
library(plyr)
library(pander)
library(sm)
library(AICcmodavg)
library(rethinking)
```

# Part 1 - Design and set-up of true experiment 


## The motivation for the planned research
(Max 250 words)

## The theory underlying the research  
(Max 250 words) Preferable based on theories reported in literature


## Research questions 
The research question that will be examined in the experiment (or alternatively the hypothesis that will be tested in the experiment)


## The related conceptual model 
This model should include:
*Independent variable(s)
*Dependent variable
*Mediating variable (at least 1)
*Moderating variable (at least 1)


## Experimental Design 
Note that the study should have a true experimental design

## Experimental procedure 
Describe how the experiment will be executed step by step


## Measures
Describe the measure that will be used

## Participants
Describe which participants will recruit in the study and how they will be recruited

## Suggested statistical analyses
Describe the statistical test you suggest to care out on the collected data

<!-- # Part 2 - Generalized linear models -->

<!-- ## Question 1 Twitter sentiment analysis (Between groups - single factor)  -->

<!-- ### Conceptual model -->
<!-- Make a conceptual model for the following research question: Is there a difference in the sentiment of the tweets related to the different celebrities? -->

<!-- ### Collecting tweets, and data preparation -->
<!-- Include the annotated R script (excluding your personal Keys and Access Tokens information), but put echo=FALSE, so code is not included in the output pdf file. -->


<!-- ```{r, echo=FALSE, message=FALSE, warning=FALSE, include = FALSE} -->

<!-- #during writing you could add "eval = FALSE",  kntr will than not run this code chunk (take some time do) -->

<!-- #setwd("~/surfdrive/Teaching/own teaching/IN4125 - Seminar Research Methodology for Data Science/2019/coursework A")  -->
<!-- # commented this. Let's make this relative when trying to fix this file -->
<!-- # apple , note use / instead of \, which used by windows -->


<!-- #install.packages("twitteR", dependencies = TRUE) -->
<!-- library(twitteR) -->
<!-- #install.packages("RCurl", dependencies = T) -->
<!-- library(RCurl) -->
<!-- #install.packages("bitops", dependencies = T) -->
<!-- library(bitops) -->
<!-- #install.packages("plyr", dependencies = T) -->
<!-- library(plyr) -->
<!-- #install.packages('stringr', dependencies = T) -->
<!-- library(stringr) -->
<!-- #install.packages("NLP", dependencies = T) -->
<!-- library(NLP) -->
<!-- #install.packages("tm", dependencies = T) -->
<!-- library(tm) -->
<!-- #install.packages("wordcloud", dependencies=T) -->
<!-- #install.packages("RColorBrewer", dependencies=TRUE) -->
<!-- library(RColorBrewer) -->
<!-- library(wordcloud) -->
<!-- #install.packages("reshape", dependencies=T) -->
<!-- library(reshape) -->

<!-- ################### functions -->


<!-- clearTweets <- function(tweets, excl) { -->

<!--   tweets.text <- sapply(tweets, function(t)t$getText()) #get text out of tweets  -->


<!--   tweets.text = gsub('[[:cntrl:]]', '', tweets.text) -->
<!--   tweets.text = gsub('\\d+', '', tweets.text) -->
<!--   tweets.text <- str_replace_all(tweets.text,"[^[:graph:]]", " ") #remove graphic -->


<!--   corpus <- Corpus(VectorSource(tweets.text)) -->

<!--   corpus_clean <- tm_map(corpus, removePunctuation) -->
<!--   corpus_clean <- tm_map(corpus_clean, content_transformer(tolower)) -->
<!--   corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("english")) -->
<!--   corpus_clean <- tm_map(corpus_clean, removeNumbers) -->
<!--   corpus_clean <- tm_map(corpus_clean, stripWhitespace) -->
<!--   corpus_clean <- tm_map(corpus_clean, removeWords, c(excl,"http","https","httpst")) -->


<!--   return(corpus_clean) -->
<!-- }  -->


<!-- ## capture all the output to a file. -->

<!-- ################# Collect from Twitter -->

<!-- # for creating a twitter app (apps.twitter.com) see youtube https://youtu.be/lT4Kosc_ers -->
<!-- #consumer_key <-'your key' -->
<!-- #consumer_scret <- 'your secret' -->
<!-- #access_token <- 'your access token' -->
<!-- #access_scret <- 'your access scret' -->

<!-- source("your_twitter.R") #this file will set my personal variables for my twitter app, adjust the name of this file. use the provide template your_twitter.R -->

<!-- setup_twitter_oauth(consumer_key,consumer_scret, access_token,access_scret) #connect to  twitter app -->


<!-- ##### This example uses the following 3 celebrities: Donald Trump, Hillary Clinton, and Bernie Sanders -->
<!-- ##  You should replace this with your own celebrities, at least 3, but more preferred  -->
<!-- ##  Note that it will take the computer some to collect the tweets -->

<!-- tweets_T <- searchTwitter("#trump", n=300, lang="en", resultType="recent") #300 recent tweets about Donald Trump, in English (I think that 1500 tweets is max) -->
<!-- tweets_C <- searchTwitter("#hillary", n=300, lang="en", resultType="recent") #300 recent tweets about Hillary Clinton -->
<!-- tweets_B <- searchTwitter("#bernie", n=300, lang="en", resultType="recent") #300 recent tweets about Bernie Sanders -->



<!-- ######################## WordCloud -->
<!-- ### This not requires in the assignment, but still fun to do  -->

<!-- # based on https://youtu.be/JoArGkOpeU0 -->

<!-- #corpus_T<-clearTweets(tweets_T, c("trump","amp","realdonaldtrump","trumptrain","donald","trumps","alwaystrump")) #remove also some campain slogans -->
<!-- #wordcloud(corpus_T, max.words=50) -->

<!-- #corpus_C<-clearTweets(tweets_C, c("hillary","amp","clinton","hillarys")) -->
<!-- #wordcloud(corpus_C,  max.words=50) -->

<!-- #corpus_B<-clearTweets(tweets_B, c("bernie", "amp", "sanders","bernies")) -->
<!-- #wordcloud(corpus_B,  max.words=50) -->
<!-- ############################## -->

<!-- ######################## Sentiment analysis -->

<!-- tweets_T.text <- laply(tweets_T, function(t)t$getText()) #get text out of tweets  -->
<!-- tweets_C.text <- laply(tweets_C, function(t)t$getText()) #get text out of tweets -->
<!-- tweets_B.text <- laply(tweets_B, function(t)t$getText()) #get text out of tweets -->



<!-- #taken from https://github.com/mjhea0/twitter-sentiment-analysis -->
<!-- pos <- scan('positive-words.txt', what = 'character', comment.char=';') #read the positive words -->
<!-- neg <- scan('negative-words.txt', what = 'character', comment.char=';') #read the negative words -->

<!-- source("sentiment3.R") #load algoritm -->
<!-- # see sentiment3.R form more information about sentiment analysis. It assigns a intereger score -->
<!-- # by substracitng the number of occurrence of negative words from that of positive words -->

<!-- analysis_T <- score.sentiment(tweets_T.text, pos, neg) -->
<!-- analysis_C <- score.sentiment(tweets_C.text, pos, neg) -->
<!-- analysis_B <- score.sentiment(tweets_B.text, pos, neg) -->


<!-- sem<-data.frame(analysis_T$score, analysis_C$score, analysis_B$score) -->


<!-- semFrame <-melt(sem, measured=c(analysis_T.score,analysis_C.score, analysis_B.score )) -->
<!-- names(semFrame) <- c("Candidate", "score") -->
<!-- semFrame$Candidate <-factor(semFrame$Candidate, labels=c("Donald Trump", "Hillary Clinton", "Bernie Sanders")) # change the labels for your celibrities -->

<!-- #The data you need for the analyses can be found in semFrame -->

<!-- ``` -->

<!-- ### Homogeneity of variance analysis -->
<!-- Analyze the homogeneity of variance of sentiments of the tweets of the different celebrities, and provide interpretation -->

<!-- ```{r} -->
<!-- #include your code and output in the document -->
<!-- ``` -->


<!-- ### Visual inspection Mean and distribution sentiments -->
<!-- Graphically examine the mean and distribution sentiments of tweets for each celebrity, and provide interpretation -->

<!-- ```{r} -->
<!-- #include your code and output in the document -->
<!-- ``` -->
<!-- ### Frequentist approach -->

<!-- #### Linear model -->
<!-- Use a linear model to analyze whether the knowledge to which celebrity a tweet relates has a significant impact on explaining the sentiments of the tweets. Provide interpretation of results  -->

<!-- ```{r} -->
<!-- #include your code and output in the document -->
<!-- ``` -->

<!-- #### Post Hoc analysis -->
<!-- If a model that includes the celebrity is better in explaining the sentiments of tweets than a model without such predictor, conduct a post-hoc analysis with e.g. Bonferroni correction, to examine which of celebrity tweets differ from the other celebrity tweets. Provide interpretation of the results -->

<!-- ```{r} -->
<!-- #include your code and output in the document -->
<!-- ``` -->

<!-- #### Report section for a scientific publication -->
<!-- Write a small section for a scientific publication, in which you report the results of the analyses, and explain the conclusions that can be drawn. -->

<!-- ### Bayesian Approach -->

<!-- #### Model description -->

<!-- Describe the mathematical model fitted on the most extensive model. (hint, look at the mark down file of the lectures to see example on formulate mathematical models in markdown). Justify the priors. -->

<!-- #### Model comparison -->

<!-- Conduct model analysis and provide brief interpretation of the results -->

<!-- ```{r} -->
<!-- #include your code and output in the document -->
<!-- ``` -->

<!-- #### Comparison celebrity pair -->

<!-- Compare sentiments of celebrity pairs and provide a brief interpretation (e.g. CIs)  -->


## Question 2 - Website visits (between groups - Two factors)

### Conceptual model
<!-- Make a conceptual model underlying this research question -->

```{r concept_diag, echo=FALSE, fig.cap="\\label{fig:concept_diag}The conceptual model underlying the research question.", out.width = '100%'}
knitr::include_graphics("img/concept_diag.png")
```

### Visual inspection
<!-- Graphically examine the variation in page visits for different factors levels (e.g. histogram, density plot etc.)  -->


```{r}
filepath <- ("webvisit0.csv")
data <- read.csv(file=filepath, header=TRUE)

# changing dtype of the factors
data$portal = as.factor(data$portal)
data$version = as.factor(data$version)

# Function to calculate the mean and the standard deviation for each factor group

data_summary <- function(data, varname, groupnames){
  require(plyr)
  summary_func <- function(x, col){
    c(mean = mean(x[[col]], na.rm=TRUE),
      sd = sd(x[[col]], na.rm=TRUE))
  }
  data_sum<-ddply(data, groupnames, .fun=summary_func,
                  varname)
  data_sum <- rename(data_sum, c("mean" = varname))
 return(data_sum)
}

df3 <- data_summary(data, varname="pages", 
                    groupnames=c("version", "portal"))

p <- ggplot(df3, aes(x=version, y=pages, fill=portal)) + 
   geom_bar(stat="identity", position=position_dodge()) +
  geom_errorbar(aes(ymin=pages-sd, ymax=pages+sd), width=.2,
                 position=position_dodge(.9))
  
p + scale_fill_brewer(palette="Paired") + theme_minimal()

# Creating subsets of data for each combination of factors
subset00 <- subset(data, version == '0' & portal == '0')
subset01 <- subset(data, version == '0' & portal == '1')
subset10 <- subset(data, version == '1' & portal == '0')
subset11 <- subset(data, version == '1' & portal == '1')


mean(subset00$pages)
mean(subset01$pages)
mean(subset10$pages)
mean(subset11$pages)

mean(data$pages)

```


### Normality check
<!-- Visually inspect if variable page visits deviates from a Gaussian distribution, and discuss implication for general linear model analysis. -->


```{r}
# Generating density plots

d <- density(data$pages)
plot(d, xlab='Number of Page Visits', ylab='Prob. Density', main='Aggregated Page visits')
abline(v = mean(data$pages), col = "black")

d <- density(subset00$pages)
plot(d, xlab='Number of Page Visits', ylab='Prob. Density', main='Page visits on Old version for Consumers entries')
abline(v = mean(subset00$pages), col = "red")

d <- density(subset01$pages)
plot(d, xlab='Number of Page Visits', ylab='Prob. Density', main='Page visits on Old version for Company entries')
abline(v = mean(subset01$pages), col = "green")

d <- density(subset10$pages)
plot(d, xlab='Number of Page Visits', ylab='Prob. Density', main='Page visits on New version for Consumers entries')
abline(v = mean(subset10$pages), col = "blue")

d <- density(subset11$pages)
plot(d, xlab='Number of Page Visits', ylab='Prob. Density', main='Page visits on New version for Company entries')
abline(v = mean(subset11$pages), col = "orange")
```

### Frequentist Approach

#### Model analysis
<!-- Conduct a model analysis, to examine the added values of adding 2 factors and interaction between the factors in the model to predict page visits, and include brief interpretation of the results. -->


```{r}
# Model fitting for each factor and a combination of them

model0 <- lm(pages ~ 1, data=data, na.action=na.exclude)
model1 <- lm(pages ~ version, data=data, na.action=na.exclude)
model2 <- lm(pages ~ portal, data=data, na.action=na.exclude)
model3 <- lm(pages ~ version + portal, data=data, na.action=na.exclude)
model4 <- lm(pages ~ version + portal + version:portal, data=data, na.action=na.exclude)

# ANOVA results of the effect of adding the factors

pander(anova(model0, model1), caption='Version as main effect on Page visits')

pander(anova(model0, model2), caption='Portal as main effect on Page visits')

pander(anova(model3, model4), caption='Interaction effect vs 2 main effects')

pander(anova(model4), caption='Version, Portal and interaction effect on Page visits')

# AICc scores of the models

models <-list(model0, model1, model2, model3, model4)
model.names <-c("model0","model1","model2","model3","model4")
pander(aictab(cand.set = models, modnames=model.names))
```

The ANOVA results for the comparison of each model type indicate that the added values by including the factors individually, together and their interaction effect is statistically significant since all their p-values are <0.001. Out of all these, the added value is the most significant in the case of just adding the factor of Portal since it's p-value is the least (1.28e-37). The AICc results show that model4 has the best goodness of fit since its corrected-AIC value is the least with the best log-likelihood score too.

#### Simple effect analysis
<!-- If the analysis shows a significant two-way interaction effect, conduct a Simple Effect analysis to explore this interaction effect in more detail.It helps first to look at the means of different conditions in a figure. Provide brief interpretation of the results. -->


```{r}
data$simple <- interaction(data$version, data$portal)
contrast0 <-c(1,-1,0,0) #Only the 0-portal data
contrast1 <-c(0,0,1,-1) #Only the 1-portal data

SimpleEff <- cbind(contrast0,contrast1)
contrasts(data$simple) <- SimpleEff

simpleEffectModel <-lm(pages ~ simple , data = data, na.action = na.exclude)
pander(summary.lm(simpleEffectModel))
```

After fitting a linear model on the data, it can be observed that the company portal entries (1) have a statistically significant difference and not the consumer portal entries (0). This observation also agrees with the first plot indicating the variation in page visits for the 2 factors. The 1-portal page visits have a larger difference than the 0-portal page visits for the 0 and 1 - versions.

#### Report section for a scientific publication
<!-- Write a small section for a scientific publication, in which you report the results of the analyses, and explain the conclusions that can be drawn. -->

A linear model was fitted on the number of page visits by users, taking website version and web portal entires as independent variables, and including a two-way interaction between these variables. The analysis found a significant main effect (F (1, 995) = 36.2, p. < 0.01) for the version factor and (F (1, 995) = 178.8, p. < 0.01) for portal factor. The analysis also found a significant two-way interaction effect ( F (1, 76) = 46.25, p. < 0.01) between these two variables. A Simple Effect analysis further examined the two-way interaction. It revealed a significant (t = 9.222, p. < 0.01) difference for the web portal entries by companies (1), but no significant effect (t = -0.4258, p. = 0.6703) was found for the web portal entries by consumers (0).

### Bayesian Approach

#### Model description

<!-- Describe the mathematical model fitted on the most extensive model. (hint, look at the mark down file of the lectures to see example on formulate mathematical models in markdown). Justify the priors. -->
A gaussian model is fitted to each of the models. Model m0 is the base model with only an intercept. Model m1 is an extension of model m0 where the version in introduced as a predictor. Model m2 is again an extension of model m0 with portal as a predictor. In model m3, both predictors are added as main effects, and model m4 extends model m3 by adding a two-way interaction effect between version and portal in the model. The priors are chosen with a normal distribution of N(0,1) for each of the model types.

$$ score \sim Norm(\mu, \sigma)$$

$$ \mu = \alpha $$

$$ alpha = Norm(0,1) $$

$$ \sigma = Uniform(0.1, 2)$$


#### Model comparison

<!-- Conduct model analysis and provide brief interpretation of the results -->


```{r}
datasub <- subset(data, select = c(pages, version, portal))
datasub$versionN <- as.numeric(datasub$version)
datasub$portalN <- as.numeric(datasub$portal)

#Fitting each variant of the model

m0 <-map2stan(    
    alist(
        pages ~ dnorm(mu, sigma),
        mu <- a ,
        a ~ dnorm(1, 2),
        sigma ~ dunif(0.1, 2)
    ), data = datasub, iter = 10000, chains = 4, cores = 4
)

m1 <-map2stan(    
    alist(
        pages ~ dnorm(mu, sigma),
        mu <- a + b*versionN ,
        a ~ dnorm(1, 2),
        b ~ dnorm(0, 1),
        sigma ~ dunif(0.1, 2)
    ), data = datasub, iter = 10000, chains = 4, cores = 4
)

m2 <-map2stan(    
    alist(
        pages ~ dnorm(mu, sigma),
        mu <- a + b*portalN ,
        a ~ dnorm(1, 2),
        b ~ dnorm(0, 1),
        sigma ~ dunif(0.1, 2)
    ), data = datasub, iter = 10000, chains = 4, cores = 4
)

m3 <-map2stan(    
    alist(
        pages ~ dnorm(mu, sigma),
        mu <- a + b*versionN + c*portalN ,
        a ~ dnorm(1, 2),
        b ~ dnorm(0, 1),
        c ~ dnorm(0, 1),
        sigma ~ dunif(0.1, 2)
    ), data = datasub, iter = 10000, chains = 4, cores = 4
)

m4 <-map2stan(    
    alist(
        pages ~ dnorm(mu, sigma),
        mu <- a + b*versionN + c*portalN + d*versionN*portalN ,
        a ~ dnorm(1, 2),
        b ~ dnorm(0, 1),
        c ~ dnorm(0, 1),
        d ~ dnorm(0, 1),
        sigma ~ dunif(0.1, 2)
    ), data = datasub, iter = 10000, chains = 4, cores = 4
)

pander(compare(m0,m1,m2,m3,m4))
pander(precis(m4, prob= .95))
```

The compare() function indicates the best goodness of fit has been observed for the model m4 with the least WAIC value. For further investigation of the 95% credibility intervals, the precis() function indicates that the mean value of the coefficient of version is approximately 0, unlike for the coefficients of all the other variables (c, d for portal and two-way interaction respectively).

<!-- # Part 3 - Multilevel model -->

<!-- ## Visual inspection -->
<!-- Use graphics to inspect the distribution of the score, and relationship between session and score -->


<!-- ```{r} -->
<!-- #include your code and output in the document -->
<!-- ``` -->

<!-- ## Frequentist approach -->

<!-- ### Multilevel analysis -->
<!-- Conduct multilevel analysis and calculate 95% confidence intervals, determine: -->

<!-- * If session has an impact on people score -->
<!-- * If there is significant variance between the participants in their score -->


<!-- ```{r} -->
<!-- #include your code and output in the document -->
<!-- ``` -->

<!-- ### Report section for a scientific publication -->
<!-- Write a small section for a scientific publication, in which you report the results of the analyses, and explain the conclusions that can be drawn. -->

<!-- ## Bayesian approach -->

<!-- ### Model description -->

<!-- Describe the mathematical model fitted on the most extensive model. (hint, look at the mark down file of the lectures to see example on formulate mathematical models in markdown). Justify the priors. -->

<!-- ### Model comparison -->

<!-- Select the first 100 participants from the data set. (hint to overcome the Stan problem with a zero index, increase subject id number with 1). Compare models with with increasing complexity.  -->

<!-- ```{r} -->
<!-- #include your code and output in the document -->
<!-- ``` -->

<!-- ### Estimates examination -->

<!-- Examine the estimate of parameters of the model with best fitt, and provide a brief interpretation. -->


<!-- ```{r} -->
<!-- #include your code and output in the document -->
<!-- ``` -->


